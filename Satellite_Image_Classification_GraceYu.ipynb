{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction. I want to classify any user image by training on availabe satellite image. The training and test data set DeepSat(SAT-4) Airborne Dataset is from https://www.kaggle.com/arpandhatt/satellite-image-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAT-4\n",
    "SAT-4 consists of a total of 500,000 image patches covering four broad land cover classes. These include - barren land, trees, grassland and a class that consists of all land cover classes other than the above three. 400,000 patches (comprising of four-fifths of the total dataset) were chosen for training and the remaining 100,000 (one-fifths) were chosen as the testing dataset. We ensured that the training and test datasets belong to disjoint set of image tiles. Each image patch is size normalized to 28x28 pixels. Once generated, both the training and testing datasets were randomized using a pseudo-random number generator.\n",
    "\n",
    "The MAT file for the SAT-4 dataset contains the following variables:\n",
    "\n",
    "train_x\t28x28x4x400000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)\n",
    "train_y\t400000x4 uint8 (containing 4x1 vectors having labels for the 400000 training samples)\n",
    "test_x\t28x28x4x100000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)\n",
    "test_y\t100000x4 uint8 (containing 4x1 vectors having labels for the 100000 test samples)\n",
    "\n",
    "SAT-6\n",
    "SAT-6 consists of a total of 405,000 image patches each of size 28x28 and covering 6 landcover classes - barren land, trees, grassland, roads, buildings and water bodies. 324,000 images (comprising of four-fifths of the total dataset) were chosen as the training dataset and 81,000 (one fifths) were chosen as the testing dataset. Similar to SAT-4, the training and test sets were selected from disjoint NAIP tiles. Once generated, the images in the dataset were randomized in the same way as that for SAT-4. The specifications for the various landcover classes of SAT-4 and SAT-6 were adopted from those used in the National Land Cover Data (NLCD) algorithm.\n",
    "\n",
    "The MAT file for the SAT-6 dataset contains the following variables:\n",
    "\n",
    "train_x\t28x28x4x324000 uint8 (containing 324000 training samples of 28x28 images each with 4 channels)\n",
    "train_y\t324000x6 uint8 (containing 6x1 vectors having labels for the 324000 training samples)\n",
    "test_x\t28x28x4x81000 uint8 (containing 81000 test samples of 28x28 images each with 4 channels)\n",
    "test_y\t81000x6 uint8 (containing 6x1 vectors having labels for the 81000 test samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "\n",
    "from math import ceil\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "\n",
    "# my add on\n",
    "from PIL import Image\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the satellite image csv file. The original file has 100000 test samples. \n",
    "x_train = pd.read_csv(\"X_train_sat4.csv\", header=None)\n",
    "x_test = pd.read_csv(\"X_test_sat4.csv\", header=None)\n",
    "y_train = pd.read_csv(\"y_train_sat4.csv\", header=None)\n",
    "y_test = pd.read_csv(\"y_test_sat4.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3126</th>\n",
       "      <th>3127</th>\n",
       "      <th>3128</th>\n",
       "      <th>3129</th>\n",
       "      <th>3130</th>\n",
       "      <th>3131</th>\n",
       "      <th>3132</th>\n",
       "      <th>3133</th>\n",
       "      <th>3134</th>\n",
       "      <th>3135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>170</td>\n",
       "      <td>157</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>147</td>\n",
       "      <td>132</td>\n",
       "      <td>202</td>\n",
       "      <td>165</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>216</td>\n",
       "      <td>165</td>\n",
       "      <td>145</td>\n",
       "      <td>127</td>\n",
       "      <td>204</td>\n",
       "      <td>169</td>\n",
       "      <td>148</td>\n",
       "      <td>129</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>108</td>\n",
       "      <td>129</td>\n",
       "      <td>19</td>\n",
       "      <td>85</td>\n",
       "      <td>114</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>116</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>89</td>\n",
       "      <td>187</td>\n",
       "      <td>195</td>\n",
       "      <td>190</td>\n",
       "      <td>94</td>\n",
       "      <td>188</td>\n",
       "      <td>193</td>\n",
       "      <td>189</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221</td>\n",
       "      <td>213</td>\n",
       "      <td>174</td>\n",
       "      <td>213</td>\n",
       "      <td>223</td>\n",
       "      <td>217</td>\n",
       "      <td>177</td>\n",
       "      <td>213</td>\n",
       "      <td>223</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>181</td>\n",
       "      <td>166</td>\n",
       "      <td>157</td>\n",
       "      <td>137</td>\n",
       "      <td>180</td>\n",
       "      <td>170</td>\n",
       "      <td>159</td>\n",
       "      <td>137</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>132</td>\n",
       "      <td>206</td>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>131</td>\n",
       "      <td>204</td>\n",
       "      <td>143</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>193</td>\n",
       "      <td>148</td>\n",
       "      <td>145</td>\n",
       "      <td>126</td>\n",
       "      <td>188</td>\n",
       "      <td>154</td>\n",
       "      <td>150</td>\n",
       "      <td>135</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>148</td>\n",
       "      <td>113</td>\n",
       "      <td>121</td>\n",
       "      <td>52</td>\n",
       "      <td>179</td>\n",
       "      <td>117</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>113</td>\n",
       "      <td>180</td>\n",
       "      <td>129</td>\n",
       "      <td>126</td>\n",
       "      <td>108</td>\n",
       "      <td>182</td>\n",
       "      <td>95</td>\n",
       "      <td>105</td>\n",
       "      <td>97</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  3126  \\\n",
       "0   190   170   157   215   172   147   132   202   165   139  ...   161   \n",
       "1    76   108   129    19    85   114   134    22   116   145  ...   176   \n",
       "2   221   213   174   213   223   217   177   213   223   214  ...   137   \n",
       "3   149   150   132   206   148   149   131   204   143   144  ...   130   \n",
       "4    62    91    18   148   113   121    52   179   117   107  ...   113   \n",
       "\n",
       "   3127  3128  3129  3130  3131  3132  3133  3134  3135  \n",
       "0   216   165   145   127   204   169   148   129   209  \n",
       "1    89   187   195   190    94   188   193   189    97  \n",
       "2   181   166   157   137   180   170   159   137   179  \n",
       "3   193   148   145   126   188   154   150   135   193  \n",
       "4   180   129   126   108   182    95   105    97   173  \n",
       "\n",
       "[5 rows x 3136 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 3136)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3136)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the training and testing data into numpy array with shape of [image no, 28,28,4]\n",
    "x_train_img = x_train.to_numpy().reshape([400000,28,28,4])\n",
    "x_test_img = x_test.to_numpy().reshape([100000,28,28,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert label y from pandas dataframe to numpy array\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2190e47a070>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_img[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYcUlEQVR4nO2dW4xkV3WG/3XOqaq+zYxnPL7FWDEX54IiYZKJFckkIUIB4xcbJCKsCDmRhXkACSQegsgDfjQRl/AQIQ3BwkQEhAQER7IAy0KyeAhiTIwvOMEXPPbgYcbXmelLVZ3LykOX0WB6/6vpS1Ur+/+kVnfXqn3OrlPnr1NV/15rmbtDCPH/n2LWExBCTAeJXYhMkNiFyASJXYhMkNiFyIRqmjvbv3+/X3zRRek7GB/PfAOnUaCINm483nm35W1bsO3IEAlmzo9LsHHveLysShrfzmOLnjNH+pgDm3hO2fBgaNs0PE7OBwCAtzRc9eeSMQvOB3ZMn37m6bW6rhc23CffLMfMrgPwOQAlgH9199vZ/S++6CJ85pOfTMabkj8DjaUPcBMc3LmCv4mpqh6Nr4zWkrH5qk/HDnpcMPWYhlHyh4YREU3T8MH1sKbxQ4cvoPHouI3r9P6bgguq9hGNLzjfdzckjz1Q1JkzZ2j83GiFxtG+RMMXXv4HyViv5joYN+n4Le//+7Op2JbfxptZCeBfALwTwBsB3GRmb9zq9oQQu8t2PrNfA+Bxd3/S3ccAvgbghp2ZlhBip9mO2C8H8Mx5/5+Y3PZrmNmtZnbMzI6dPZt8hyGE2GW2I/aNPjj8xgchdz/q7kfc/cj+/fu3sTshxHbYjthPALjivP9fA+DZ7U1HCLFbbEfsPwJwlZm91sz6AN4L4K6dmZYQYqfZsvXm7o2ZfQjAd7Fuvd3h7o+wMWVRYXHxwmR8VHA7ZFyvJmN1fY6OhXM7ox5zi6qt07bfSpueFwCcWeUW0mJ1kMYr49aeE/es67gfbD1+XJaH/HuWxcUlvn1meXb8mI+D44aSny9GDOl2xC3HGjw+CGzDBvy4LFx4OBnzk8/RsS3RiZHFB9vy2d39bgB3b2cbQojpoOWyQmSCxC5EJkjsQmSCxC5EJkjsQmSCxC5EJkw1n929Q0NSRds+TwVturS3WY+5L9rvc6+6LPih6JPw6pCnQ3rwktoG6Za9im+gbNPjuyhZvgrSKcfp5wsAgsxhLM7vS8aaNZ7b25L0WADonI9n6b1NsO1Bf0DjwxdP0/jSpVfS+OrPjydjB4K1C6Phy8lYgS75hOrKLkQmSOxCZILELkQmSOxCZILELkQmSOxCZMJUrbfOOwzHy8l4U6XL6wJA06RTHi1KYW15vDD+ujc/N58OdjwVcyVIEy0Xg5LKxm3FlpSDtsDW6wflnEer3HprxjzVc0Sel6CKNeqgnPNqUOG1N0qPX0g7VACAa95+I43/5Lv/SePDUWDHkuvseMht4hdOpyvXVlUveaLqyi5EJkjsQmSCxC5EJkjsQmSCxC5EJkjsQmSCxC5EJkzXZ+9arA3T3ujaypCO7/fTXTvnetyjj9Ilm5bHS+IXD5Z4p5ug6zHKoHttEfVs7qfnXpT8Ke6ilsvtht1/f0UT5O/Wlt6/B/mxC4t87i+eeYHGMU6fT/2D6ZLmAMKWzmVF1l0gLm3eW0qn0HZt0L12kN73/NxccsO6sguRCRK7EJkgsQuRCRK7EJkgsQuRCRK7EJkgsQuRCVP12WFAV6RbCLcrvDQwS0G2kucAA0FL5jZowdukd96reWL2/OIijUfVntuW57P3Bmkjvwvy9AtLr10AgLoK1j4YL7nMtj83z73qiJdWuJfdkfrfz596go5dXuPbro23wq6DNtsg6w+KltcQOLyQXlPSI4s2tiV2M3sKwDmsK6lx9yPb2Z4QYvfYiSv7X7n78zuwHSHELqLP7EJkwnbF7gC+Z2b3m9mtG93BzG41s2NmduzcWf45SAixe2z3bfy17v6smV0M4B4z+x93v+/8O7j7UQBHAeB1r7sy+CpKCLFbbOvK7u7PTn6fBvAtANfsxKSEEDvPlsVuZotmtu+VvwG8HcDDOzUxIcTOsp238ZcA+Jat50NXAP7d3b/DBpRFgX37SR4vackMAGU//drU8XR2rA5XabwK8rqLNu1lV4Gn2gXtoJtzPH+5bfhrcutpz7e3wH3wIJUec0GePyxom0x8flvjXnUX9AJYCnz6gqw/qPalW0kDwM9++gCNLwf56kinlQMAVlbT48ek5gMA/OW7/zYZW/jUPydjWxa7uz8J4E1bHS+EmC6y3oTIBIldiEyQ2IXIBIldiEyQ2IXIhCmnuBqKXjrl0cHTKccr6VTPxTle8ni+x+2xug1SFhtibwVjq4bbVwVJxQSALkintC4d9yB116NS0uMg7Rh8+y0Z3xQ8vbYITs/BHH9O+4vpc2IuOB+WX36OxnsXBF5vHZxPq2l7bRykuP7Xd76ejC2/nC6vrSu7EJkgsQuRCRK7EJkgsQuRCRK7EJkgsQuRCRK7EJkw9ZbNw7PLyfgFb3gDHf/STx9LxooRL7c8txj4okOeZto06e1bF5SS7vF0x7oM2kVzOxrjJj33wEZHs5x+PgCgDdYQrC6f5TvYd0EytBCUyO5X/FrUjfgaAPTS473ip34R+PCVB+sXhi/ReH8unX77+295Bx3bnk7Xd+0P0ue5ruxCZILELkQmSOxCZILELkQmSOxCZILELkQmSOxCZMJUfXZvWozOpEvoNuClgdsu7X2OwlLQ3C9GULbYkPZFW9LOGQBGI+6je3rT6/EuKOdMwuOghHa7xssWd+T5AoB+y9cY/OmfvzMZ++/v/gcd2wY54xbk+WOcjjct9+jbOsjjDxY/DPZdQuNO1lYcP/0sH3synbN+5uy5ZDK8ruxCZILELkQmSOxCZILELkQmSOxCZILELkQmSOxCZMJ068YXJTBYSobXXua50UWZnm4ZJW4HtAUf35Kc9bbjY2vi9wJAV/Lc6OiRNSSXf7TK89WbwIcftHzvf/KOd9P42ePHkzGveT57fY57/NUizzkHaaXdrEWtpnl88dLfofGVEy/S+JnRy8nYyy/8ko69ePHSZGytTi8QCK/sZnaHmZ02s4fPu+2Qmd1jZo9Nfh+MtiOEmC2beRv/JQDXveq2jwG4192vAnDv5H8hxB4mFLu73wfg1e9JbgBw5+TvOwHcuMPzEkLsMFv9gu4Sdz8JAJPfF6fuaGa3mtkxMzt2LvgMJoTYPXb923h3P+ruR9z9yL59+3Z7d0KIBFsV+ykzuwwAJr9P79yUhBC7wVbFfheAmyd/3wzg2zszHSHEbhH67Gb2VQBvBXDYzE4A+ASA2wF83cxuAfA0gPdsam8GeJX2q8t53mO9KtOJ3xb45DDu6c6XfPxwIV37fTTiXnZdn6FxC2qU94LHVpCc8rk5/nruRXAKkNrrAHD8sftp/NTPH0/Giv2H6FgP9j2qgnz2bpgMlSU/5kXH89VXfnmK7zu4jJaWXlvRB68RUBXpeNPUyQ2HYnf3mxKht0VjhRB7By2XFSITJHYhMkFiFyITJHYhMkFiFyITpltK2jvUXTp1sOm4PVYupNv/9ok1BgDDjr+uDUc81dPHJA01KDvcH3CbZzRMW0QAUBV87nNL6ZWJhfE61d7nc2uDNNTlFW4r1kjbY72gJXMTlNju9bg95g2x5oJzbW6Or/asR/w5a4yX6K6I3Xro0IV8bJ22YguS6q0ruxCZILELkQkSuxCZILELkQkSuxCZILELkQkSuxCZMFWf3YoC1QJpwxv41YN+2kP0IEW1qPnrWmX8UAzrtG8aZJHCGt5y+cASL87rQ+7ZLiyly3OXztMlu6DtcdTa+OUX0yWRAcAW055xOc89fnM+tyroZF2S1OD+Ik+nBmkPDgBO1g8AAMk0XY8Tn9/GfA1ARdqPF4Ulz0Zd2YXIBIldiEyQ2IXIBIldiEyQ2IXIBIldiEyQ2IXIhKn67GVR4eBc2lMeVmt0vDdp33R0hnvRZceTo+dImWoAmDuUzqUvWu6LNqTUMwCstDw3ugjWH8yvpY+bgz+u2rkf3Bo3s0dVkHS+mD7FGufb7hV82yVpyQwAC3PpNR1zg3k6dnmFP6cdaeENAMQKBwAMqvT+u5pve7CYflydpxcn6MouRCZI7EJkgsQuRCZI7EJkgsQuRCZI7EJkgsQuRCZM1WcHgC6dbgtUJNcdQNuM0jFui2IQ5Ktb0B64JLXbveTbbtd4TfrVcy/Q+MKA517XbdqvroNc+jbIGR8b9/jbLjCUCVXF674XFsW5z14V6fOpcL7ttgnWfHR8fUIVtcIm6z76Pb6+YGEpfT40bboAQXhlN7M7zOy0mT183m23mdkvzOyByc/10XaEELNlM2/jvwTgug1u/6y7Xz35uXtnpyWE2GlCsbv7fQBenMJchBC7yHa+oPuQmT04eZufXPBuZrea2TEzO3bmzNlt7E4IsR22KvbPA3g9gKsBnATw6dQd3f2oux9x9yMHDuzf4u6EENtlS2J391Pu3vp6hs0XAFyzs9MSQuw0WxK7mV123r/vAvBw6r5CiL1B6LOb2VcBvBXAYTM7AeATAN5qZlcDcABPAfjAZnbmZqhJjnIXeJPNmPR2B/eDO/C68mXgu3ZO8rJHaf9/fSz3TYs+7y3fBL7rKvN8g8TqgqxdAIAqqMd/8NLLafyFJx9Lxqzj15o6WPswsuC4k5xzC9Y+rAU1BoysbQCAXnDc3NJz6wXrB4qWnKtdesOh2N39pg1u/mI0Tgixt9ByWSEyQWIXIhMkdiEyQWIXIhMkdiEyYaopru5OW9n2B4EFVaQtCRJajwcprl2Qqml1Ood2bY2XsW6d598usDbWACxI9WyatI3TD1o2O0ndBYAuiL/wzBM0XvbTxz3oFo0qsEvroIr1uCYp0QhKQQ/44y7ItgFg3vkJWfbIuR5cglueXZs8aLqyC5EJErsQmSCxC5EJErsQmSCxC5EJErsQmSCxC5EJ0y0l3XXwtbQ/2evzNrpNkfabqz5/3YpKJlvDzcvhctpLrwbcU22CNNJBnxvG4477zUbKRVvBH3fT8n33DlxM48tPPEDjl7/pTemxT/+SjrWGpy1XpCUzAAxH6TTWZo2nsNZDnsK6wHxyAAU/nVCStsu2ELWTXk6PLSz5hOrKLkQmSOxCZILELkQmSOxCZILELkQmSOxCZILELkQmTL1ls7E84o57myWxjLsgX33/gX003qzynHSr0ztfCcZ6FRzmkuero+W5102Tnlvh3KOP8vhHz3EvvFw4ROPPH382GdsXrKsIpo42uFb1+sTL5qca6uYcjR9+ze/R+Opxnuc/HqWPe1kF5b+dPG5PC0xXdiEyQWIXIhMkdiEyQWIXIhMkdiEyQWIXIhMkdiEyYao+u8FQpdNt0QY55f25tB9tpBU0ABhp37s+t6Ctcpk+VHzLQL/iPnoZtOhFUHeedWX2im+71wtaC4N7vh6tESjT15NqfoEO7cDN8GG9RuMD9th5OjqWFvgdzp56msab5XTOOQD0nK0x4M932Q8WICQIr+xmdoWZfd/MHjWzR8zsw5PbD5nZPWb22OT3wS3NQAgxFTbzNr4B8FF3/0MAfwbgg2b2RgAfA3Cvu18F4N7J/0KIPUoodnc/6e4/nvx9DsCjAC4HcAOAOyd3uxPAjbs1SSHE9vmtvqAzsysBvBnADwFc4u4ngfUXBAAbFiszs1vN7JiZHTtz9uz2ZiuE2DKbFruZLQH4BoCPuPumVevuR939iLsfObB//1bmKITYATYldltvI/oNAF9x929Obj5lZpdN4pcBOL07UxRC7ASh9WZmBuCLAB5198+cF7oLwM0Abp/8/vYmtoU+KcE7Cqy3ai5tjxVBGqkH2+5abndUZXrfi4vcQip63NYrg5bMgXOH3iD92AZzQcnj4PV+SEp/A8CBg0t8++SxrzzH7akuSvUMjmtD0nfboDx3M+TnQzAc/cOX0Hjh6eelR841ALAivfPC0rHN+OzXAngfgIfM7JUi4R/Husi/bma3AHgawHs2sS0hxIwIxe7uP0C6wfvbdnY6QojdQstlhcgEiV2ITJDYhcgEiV2ITJDYhciE6ZaSNoP30q8vbcPNy1GdTia1EW/BuzrmvulCn/vR1SAdr4K2yEWUBFvz1sTmfHxF1hh0wVgP4r3A5DdW1hhAR56zqFQ0iGe8Hub7rpv0OdGRNtcAUAYp02WwrqPjw9EjKdXERl/HiA4s6Zzpyi5ELkjsQmSCxC5EJkjsQmSCxC5EJkjsQmSCxC5EJkzVZ3cAzJYdEC8bAFZX0rnVddB6eNxxn90C33SuSMd7FZ9326zy+BoviWzBcen30l54cFhCn71I27YAgDrI+2bDPSpjXW7PZ2+Jld42/HEvXcBrFKyt8ZbOg94ijVfkwHjH112wJQKkY7Ou7ELkgsQuRCZI7EJkgsQuRCZI7EJkgsQuRCZI7EJkwlR99rIwLA3SbXSHQ16jvBul/cd+4JMfvuBCPjcaBZwY1nVQW70d85r14xH34eeDnPKqSnu2Z1dX6NhBn/vJVdDyuW64J+yeNoWXlnjN+ZYZ5QDqoBcA2TXgfAFCfY776P1gDQAp5Q8A6FpyzpB8dQAoBuRcJ0N1ZRciEyR2ITJBYhciEyR2ITJBYhciEyR2ITJBYhciEzbTn/0KAF8GcCmADsBRd/+cmd0G4P0Anpvc9ePufjfbVtd1WCNeej3mtd+N1GcfMO8RQL/iTno95Dnl7TjtJzdBzfqWGr6AB33GPahhXrfp4+JlkPMd1LSvgriDP7aO1BGogmtNL3rOgl4AFelz3pW8RsB4JVj7QGoIAEAv6C3ftqT2e59vuyzScff0AoLNLKppAHzU3X9sZvsA3G9m90xin3X3T21iG0KIGbOZ/uwnAZyc/H3OzB4FcPluT0wIsbP8Vp/ZzexKAG8G8MPJTR8yswfN7A4zO5gYc6uZHTOzY2fOnN3WZIUQW2fTYjezJQDfAPARdz8L4PMAXg/gaqxf+T+90Th3P+ruR9z9yIED+3dgykKIrbApsZtZD+tC/4q7fxMA3P2Uu7eTLwS+AOCa3ZumEGK7hGI3MwPwRQCPuvtnzrv9svPu9i4AD+/89IQQO8Vmvo2/FsD7ADxkZg9Mbvs4gJvM7GqsJ9U9BeAD0YbcHaM6bZeMSYtdAOjNpdMtB/2gxW7Q/rfuuIU0GqVTHts6aP+7ME/jVW+OxodBqWrWltn7/CluOp6LaTWP9wqeAluQ1OPh8AwdW/b5tvft48dtjliaoxG37VbG/DlFxa+TUenyokfabIOnDRckIXtycd6QzXwb/wNsXP2beupCiL2FVtAJkQkSuxCZILELkQkSuxCZILELkQkSuxCZMP2WzSS1rwzSDnvEX+wanlI47HhJ5caD2r/EV52f5z56G6SoVkGp6OU17tk2pKRyv8dfz6MS2nXNPd9ywLfAfHYE7aQRrH0oCr52oiAprkWQgooykAZJMwWANihV3ZBU7yi11ytyPpA1F7qyC5EJErsQmSCxC5EJErsQmSCxC5EJErsQmSCxC5EJ5sSX2/GdmT0H4Ph5Nx0G8PzUJvDbsVfntlfnBWhuW2Un5/a77n7RRoGpiv03dm52zN2PzGwChL06t706L0Bz2yrTmpvexguRCRK7EJkwa7EfnfH+GXt1bnt1XoDmtlWmMreZfmYXQkyPWV/ZhRBTQmIXIhNmInYzu87M/tfMHjezj81iDinM7Ckze8jMHjCzYzOeyx1mdtrMHj7vtkNmdo+ZPTb5vWGPvRnN7TYz+8Xk2D1gZtfPaG5XmNn3zexRM3vEzD48uX2mx47MayrHbeqf2c2sBPAzAH8N4ASAHwG4yd1/OtWJJDCzpwAccfeZL8Aws78AsAzgy+7+R5Pb/gnAi+5+++SF8qC7/8MemdttAJZn3cZ70q3osvPbjAO4EcDfYYbHjszrbzCF4zaLK/s1AB539yfdfQzgawBumME89jzufh+AF1918w0A7pz8fSfWT5apk5jbnsDdT7r7jyd/nwPwSpvxmR47Mq+pMAuxXw7gmfP+P4G91e/dAXzPzO43s1tnPZkNuMTdTwLrJw+Ai2c8n1cTtvGeJq9qM75njt1W2p9vl1mIfaPCYXvJ/7vW3f8YwDsBfHDydlVsjk218Z4WG7QZ3xNstf35dpmF2E8AuOK8/18D4NkZzGND3P3Zye/TAL6FvdeK+tQrHXQnv0/PeD6/Yi+18d6ozTj2wLGbZfvzWYj9RwCuMrPXmlkfwHsB3DWDefwGZrY4+eIEZrYI4O3Ye62o7wJw8+TvmwF8e4Zz+TX2ShvvVJtxzPjYzbz9ubtP/QfA9Vj/Rv4JAP84izkk5vU6AD+Z/Dwy67kB+CrW39bVWH9HdAuACwHcC+Cxye9De2hu/wbgIQAPYl1Yl81obm/B+kfDBwE8MPm5ftbHjsxrKsdNy2WFyAStoBMiEyR2ITJBYhciEyR2ITJBYhciEyR2ITJBYhciE/4Psr8eea0163gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n"
     ]
    }
   ],
   "source": [
    "# Examine the image. The color channel are R,G,B, and I(Infrared)\n",
    "#Type a image number between 0 and 99,999 inclusive\n",
    "ix = 1000\n",
    "plt.imshow(x_train_img[ix,:,:,0:4])\n",
    "plt.show()\n",
    "#Tells what the image is\n",
    "if y_train_np[ix,0] == 1:\n",
    "    print ('Barren Land')\n",
    "elif y_train_np[ix,1] == 1:\n",
    "    print ('Trees')\n",
    "elif y_train_np[ix,2] == 1:\n",
    "    print ('Grassland')\n",
    "else:\n",
    "    print ('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the data and model ready, there is one more thing we have to do. In neural networks, it is very important we normalize training data. This means we make the mean 0, and the standard deviation 1 for the best results. However, dividing the image by 255 is good enough. We will just divide the array by 255:\n",
    "x_train_norm = x_train / 255\n",
    "x_test_norm = x_test / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1 Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(3136,)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 1.3509 - accuracy: 0.3573 - val_loss: 1.3529 - val_accuracy: 0.3508\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 1.3501 - accuracy: 0.3573 - val_loss: 1.3529 - val_accuracy: 0.3508\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 1.3501 - accuracy: 0.3573 - val_loss: 1.3529 - val_accuracy: 0.3508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2199a976880>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "model.fit(x_train_norm, y_train_np, validation_split=0.2, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2 Convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x2190edc5c10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(10, kernel_size=3, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(28, 28, 4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img_norm = x_train_img / 255\n",
    "x_test_img_norm = x_test_img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.2636 - accuracy: 0.9138 - val_loss: 0.3319 - val_accuracy: 0.9020\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 0.1755 - accuracy: 0.9433 - val_loss: 0.1561 - val_accuracy: 0.9475\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 0.1434 - accuracy: 0.9530 - val_loss: 0.1258 - val_accuracy: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2190ee14520>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(x_train_img_norm, y_train_np, validation_split=0.2, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 5s 2ms/step - loss: 0.1287 - accuracy: 0.9579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12868544459342957, 0.957859992980957]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict model on testing data\n",
    "model.evaluate(x_test_img_norm, y_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3 GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H, W, C = x_train_img_norm.shape\n",
    "D = H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the generator model\n",
    "def build_generator(latent_dim):\n",
    "    i = Input(shape=(latent_dim,))\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(1024, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(D, activation='tanh')(x)\n",
    "    \n",
    "    model = Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the discriminator model\n",
    "def build_discriminator(img_size):\n",
    "    i = Input(shape=(img_size,))\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(i,x)\n",
    "    return model\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models in preparation for training\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(D)\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002, 0.5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Create an input to represent noise sample from latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "\n",
    "# Pass noise through generator to get an image\n",
    "img = generator(z)\n",
    "\n",
    "# Make sure only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Take true output is fake, but we label them real!\n",
    "fake_pred = discriminator(img)\n",
    "\n",
    "# Create the combined model object\n",
    "combined_model = Model(z, fake_pred)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "\n",
    "# Config\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "sample_period = 200 # every 'sample_period' steps generate and save some data\n",
    "\n",
    "# Create batch labels to use when calling train_on_batch\n",
    "ones = np.ones(batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "    os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate a grid of random samples from the generator and save them to a file\n",
    "def sample_images(epoch):\n",
    "    rows, cols = 5, 5\n",
    "    noise = np.random.randn(rows * cols, latent_dim)\n",
    "    imgs = generator.predict(noise)\n",
    "    \n",
    "    # Rescale images 0 - 1\n",
    "    imgs = 0.5 * imgs +0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            idx += 1\n",
    "    fig.savefig(\"gan_images/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([290835, 101921,  38503,   7698,  45336, 145650, 121771,  56450,\\n             90826,  10515, 352751, 254207, 222436, 377072, 160185, 209357,\\n            121035, 235592, 113717, 382859,  88289,  78608, 348537, 119885,\\n             65498, 356758,  16575, 181998,  51632, 276119,  89161, 319243],\\n           dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-666a14a61bb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Select a random batch of images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mreal_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Generate fake images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TestEnv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TestEnv\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m         self._validate_read_indexer(\n\u001b[0m\u001b[0;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         )\n",
      "\u001b[1;32m~\\.conda\\envs\\TestEnv\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([290835, 101921,  38503,   7698,  45336, 145650, 121771,  56450,\\n             90826,  10515, 352751, 254207, 222436, 377072, 160185, 209357,\\n            121035, 235592, 113717, 382859,  88289,  78608, 348537, 119885,\\n             65498, 356758,  16575, 181998,  51632, 276119,  89161, 319243],\\n           dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    ## Train discriminator##\n",
    "    \n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, x_train_norm.shape[0], batch_size)\n",
    "    real_imgs = x_train_norm[idx]\n",
    "    \n",
    "    # Generate fake images\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    \n",
    "    #Train the dsicriminator\n",
    "    # both loss and accuracy are returned\n",
    "    d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
    "    d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_acc = 0.5 * (d_acc_real  + d_acc_fake)\n",
    "    \n",
    "    \n",
    "    ## Train generator\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = combined_model.train_on_batch(noise, ones)\n",
    "    \n",
    "    # Save the losses\n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "              \n",
    "    if epoch % sample_period == 0:\n",
    "        sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_losses, label='g_losses')\n",
    "plt.plot(d_losses, label='d_losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lls gan_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "a = imread('gan_images/0.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
